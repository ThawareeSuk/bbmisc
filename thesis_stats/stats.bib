
@article{davies_dont_2015,
	title = {Don't let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring)},
	copyright = {© 2015 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {2045-7758},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/ece3.1782/abstract},
	doi = {10.1002/ece3.1782},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects where treatments are not replicated and/or replicates are not statistically independent. It is a genuine but controversial issue in ecology particularly in the case of costly landscape-scale manipulations, behavioral studies where ethics or other concerns may limit sample sizes, ad hoc monitoring data, and the analysis of natural experiments where chance events occur at a single site. Here key publications on the topic are reviewed to illustrate the debate that exists about the conceptual validity of pseudoreplication. A survey of ecologists and case studies of experimental design and publication issues are used to explore the extent of the problem, ecologists’ solutions, reviewers’ attitudes, and the fate of submitted manuscripts. Scientists working across a range of ecological disciplines regularly come across the problem of pseudoreplication and build solutions into their designs and analyses. These include carefully defining hypotheses and the population of interest, acknowledging the limits of statistical inference and using statistical approaches including nesting and random effects. Many ecologists face considerable challenges getting their work published if accusations of pseudoreplication are made – even if the problem has been dealt with. Many reviewers reject papers for pseudoreplication, and this occurs more often if they haven't experienced the issue themselves. The concept of pseudoreplication is being applied too dogmatically and often leads to rejection during review. There is insufficient consideration of the associated philosophical issues and potential statistical solutions. By stopping the publication of ecological studies, reviewers are slowing the pace of ecological research and limiting the scope of management case studies, natural events studies, and valuable data available to form evidence-based solutions. Recommendations for fair and consistent treatment of pseudoreplication during writing and review are given for authors, reviewers, and editors.},
	language = {en},
	urldate = {2015-11-08},
	journal = {Ecology and Evolution},
	author = {Davies, G. Matt and Gray, Alan},
	month = oct,
	year = {2015},
	keywords = {Bayesian statistics, confounded effects, hypothesis formation, nesting, peer review, P-values, random effects, scientific publication, statistical population},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/QJ7G6A2R/Davies and Gray - 2015 - Don't let spurious accusations of pseudoreplicatio.pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/2CXDI6CQ/abstract.html:text/html}
}

@article{hurlbert_pseudoreplication_1984,
	title = {Pseudoreplication and the Design of Ecological Field Experiments},
	volume = {54},
	issn = {0012-9615},
	url = {http://www.esajournals.org/doi/abs/10.2307/1942661},
	doi = {10.2307/1942661},
	abstract = {Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27\% of them, or 48\% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing pre—layout (or conventional) and layout—specific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved.  See full-text article at JSTOR},
	number = {2},
	urldate = {2015-11-08},
	journal = {Ecological Monographs},
	author = {Hurlbert, Stuart H.},
	month = jun,
	year = {1984},
	pages = {187--211},
	file = {Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/5Z5X68SZ/1942661.html:text/html}
}

@article{student_errors_1927,
	title = {Errors of Routine Analysis},
	volume = {19},
	copyright = {Copyright © 1927 Biometrika Trust},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2332181},
	doi = {10.2307/2332181},
	number = {1/2},
	urldate = {2015-11-08},
	journal = {Biometrika},
	author = {{Student}},
	month = jul,
	year = {1927},
	pages = {151--164}
}

@article{gelman_beyond_2014,
	title = {Beyond Power Calculations: Assessing Type {S} (Sign) and Type {M} (Magnitude) Errors},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/content/9/6/641},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2015-11-08},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	pmid = {26186114},
	keywords = {design calculation, exaggeration ratio, power analysis, replication crisis, statistical significance, Type M error, Type S error},
	pages = {641--651},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/HQW2F8VX/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign) .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/4QM77D73/641.html:text/html}
}


@article{simmons_false-positive_2011,
	title = {False-Positive Psychology: Undisclosed Flexibility  in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/11/1359},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2015-11-08},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
	file = {Full Text PDF:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/TNSUZHFS/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility .pdf:application/pdf;Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/BATW66XJ/1359.html:text/html}
}


@article{gelman_difference_2006,
	title = {The Difference Between "Significant" and "Not Significant" is not Itself Statistically Significant},
	volume = {60},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313006X152649},
	doi = {10.1198/000313006X152649},
	language = {en},
	number = {4},
	urldate = {2015-11-10},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Stern, Hal},
	month = nov,
	year = {2006},
	pages = {328--331},
	file = {signif4.pdf:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/I64MSHDR/signif4.pdf:application/pdf}
}


@article{mccullough_accuracy_2008,
	title = {On the accuracy of statistical procedures in {Microsoft} {Excel} 2007},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947308001606},
	doi = {10.1016/j.csda.2008.03.004},
	abstract = {Excel 2007, like its predecessors, fails a standard set of intermediate-level accuracy tests in three areas: statistical distributions, random number generation, and estimation. Additional errors in specific Excel procedures are discussed. Microsoft’s continuing inability to correctly fix errors is discussed. No statistical procedure in Excel should be used until Microsoft documents that the procedure is correct; it is not safe to assume that Microsoft Excel’s statistical procedures give the correct answer. Persons who wish to conduct statistical analyses should use some other package.},
	number = {10},
	urldate = {2015-11-10},
	journal = {Computational Statistics \& Data Analysis},
	author = {McCullough, B. D. and Heiser, David A.},
	month = jun,
	year = {2008},
	pages = {4570--4578},
	file = {ScienceDirect Snapshot:/Users/bolker/Library/Application Support/Firefox/Profiles/rxerw03y.default/zotero/storage/HWCKWBSJ/S0167947308001606.html:text/html}
}
