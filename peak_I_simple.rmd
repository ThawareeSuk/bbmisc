---
title: "Flattening, slowing, or shrinking the curve?"
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r pkgs,message=FALSE}
library(deSolve)
library(ggplot2); theme_set(theme_bw())
library(tidyr)
library(dplyr)
library(purrr)
library(colorspace)
library(viridis)
library(emdbook)
library(cowplot)
library(directlabels)
## sudo apt-get install  libavfilter-dev
## sudo add-apt-repository -y ppa:cran/ffmpeg-3
## sudo apt-get update
## sudo apt-get dist-upgrade ## ????

## https://github.com/ropensci/av/issues/6
```

```{r params}
base_R0 <- 2
double <- 6
r <- double/log(2)
## doubling time = 6 days = 0.7/r
## r = 8.6
## b-g = 8.6
## b/g = 2
## R0*g-g=r
## g*(R0-1)=r
## g= r/(R0-1) = 8.6
base_gamma <- r/(base_R0-1)
max_time <- 5*base_gamma
base_decr <- 0.8 ## baseline decrease in R0
```

```{r defs}
sirgrad <- function(t,y,p) {
    g <- with(as.list(c(y,p)),
    {
        c(S=-R0*gamma*S*I,
          I=gamma*I*(R0*S-1),
          R=gamma*I)
    })
    return(list(g))
}
calc_sir <- function(R0=2,
                     gamma=1,
                     X0=c(S=0.995,I=0.005,R=0),
                     nt=101,
                     times=seq(0,max_time,length=nt)) {
  r1 <- ode(y=X0,
            func=sirgrad,
            times=times,
            parms=c(R0=R0,gamma=gamma))
  r2 <- (r1 %>% as.data.frame()
    %>% as_tibble()
    %>% pivot_longer(-time, names_to="var")
  )
  return(r2)
}
```
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\rone}{{\cal R}_0}

\usepackage{amsmath}

[Flatten-the-curve](https://ourworldindata.org/coronavirus#flattening-the-curve) is a [pervasive](https://twitter.com/hashtag/FlattenTheCurve) and useful idea. 
It explains why it's important to take steps to limit the spread of Coronavirus even if we don't think we can stop it from spreading around the world:
even if we can't stop the epidemic, epidemic control measures can reduce the number of severely ill patients at the peak of the epidemic, so that they can be taken care of by the limited resources (e.g., ICU beds) available.

But if you like to think mechanistically, you may wonder how (or if) it's possible that interventions that can't limit the long-term reach of the epidemic can limit the peak size.

Very simple models of disease spread, whose basic ideas go back a century, illuminate the sense in which this intuition is partly right, but also importantly wrong.

It is right in the sense that the interventions such as social distancing will reduce transmission and thus flatten the epidemic peak. It is wrong in the sense that limiting transmission will not just spread and slow the epidemic; it is also expected to limit the total size of the outbreak.

But #FlattenTheCurve is nonetheless important because **epidemic control measures have a much stronger effect on peak height than on total epidemic size**.

```{r funs}
peak_I <- function(R0=base_R0,i0=0,s0=1-i0) {
    C <- i0-1/R0*log(s0) + s0
    log(1/R0)/R0-1/R0+C
}
finalsize <- function(R0=base_R0) {
  1+1/R0*lambertW(-R0*exp(-R0))
}
cmpfun <- function(fun=peak_I,R0=base_R0,decr=base_decr) {
  round(100*(1-fun(R0*decr)/fun(R0)))
}
peak_t <- function(R0=base_R0,gamma=1) {
  tt <- (calc_sir(R0=R0,gamma=gamma,nt=501)
    %>% filter(var=="I")
    %>% filter(value==max(value))
    %>% pull(time)
  )
  return(tt)
}
Peak_t <- Vectorize(peak_t,"R0")
```

```{r peak_size_calc,cache=TRUE}
R0vec2 <- seq(1.1,base_R0,length=101)
names(R0vec2) <- R0vec2
dd <- (bind_rows(list(peak_I=tibble(R0=R0vec2,val=peak_I(R0vec2)),
                     final_size=tibble(R0=R0vec2,val=finalsize(R0vec2)),
                     peak_t=tibble(R0=R0vec2,val=Peak_t(R0vec2))),
                 .id="metric")
)
```

```{r mutate}
dd <- (dd
  %>% mutate(reduce=(1-R0/base_R0))
  %>% group_by(metric)
  %>% mutate(rel_val=val/max(val))
)
```

```{r peak_size_compare_plot}
ggplot(dd,aes(R0,val))+geom_line(aes(colour=metric))+
  facet_wrap(~metric,scale="free_y")+theme(legend.pos="none") +
  scale_x_reverse() + scale_y_continuous(name="",limits=c(0,NA)) +
  geom_vline(xintercept=base_R0*base_decr,lty=2)
```

```{r peak_size_compare_plot_2}
dd2 <- (dd
  %>% filter(metric!="peak_t")
  %>% ungroup()
  %>% mutate(metric=forcats::fct_recode(factor(metric),
                         final="final_size",peak="peak_I"))
)
(ggplot(dd2,
        aes(reduce,rel_val))
  + geom_line(aes(colour=metric),lwd=2)
  + geom_dl(method="last.bumpup",aes(label=metric,x=reduce+0.01))
  + scale_colour_discrete_qualitative()
  + scale_x_continuous(
      name="Reduction in transmission\n(strength of social distancing)",
      label=scales::label_percent(accuracy=1))
  + scale_y_continuous(limits=c(0,NA),
                       name="Reduction in cases",
                       label=scales::label_percent())
  + geom_vline(xintercept=1-base_decr,lty=2)
  + theme(legend.position="none")
  + expand_limits(x=0.5)
)
```


For example: starting from $\rzero=`r base_R0`$, a 20% decrease in $\rzero$ [NOTE this is 40% of the way to full control at $\rzero=1$] leads to a `r cmpfun()`% decrease in the epidemic peak (and a `r -cmpfun(peak_t)`% increase in the time until the peak) but only a `r cmpfun(finalsize)`% decrease in final size.


## fixme

- secondary x-axis showing relative amount of control/$\rzero$? 
- Is drop in peak time artefactual?
- Not that it matters, but is there a more efficient way to numerically solve for the peak time?
- Banking/45 degree slope in peak I?
- Check peak time calculation, numbers seem weird.


## to include?

- plot 1 showing 'flatten the curve' plot with actual SIR solutions?
- Mention [Smaldino](http://smaldino.com/wp/covid-19-modeling-the-flattening-of-the-curve/), [dsparks](https://dsparks.wordpress.com/2020/03/12/flattening-the-curve/) models? 
- Discuss timing of interventions?
- cite Shea/Ebola paper on outcome criteria?
- more refs: https://twitter.com/trvrb/status/1237934525281259521

## animation

Not finished:

- opposite direction (increasing control/decreasing $\rzero$)
- include $\rzero=2$ curve as fixed reference
- fill with 'final' colour from previous
- reference line 

```{r animation_calc}
library(gganimate)
all_I <- (purrr::map_dfr(R0vec2,calc_sir,.id="R0")
  %>% filter(var=="I")
  %>% mutate(R0=as.numeric(R0))
)
gg0 <- ggplot(all_I,aes(time,value))+geom_line(aes(colour=R0,group=R0)) +
  scale_color_gradient(high="red",low="blue")+theme_classic() 
```

```{r animate_render,eval=FALSE}
if (require("gifski") && require("av")) {
    ## animate it over years
  gg1 <- gg0 + gganimate::transition_states(R0,
                      transition_length = 1, state_length = 1) +
    gganimate::ease_aes('cubic-in-out') +
    ggtitle('R0 = {closest_state}' )

  animate(gg1, renderer=av_renderer())
  anim_save("R0.gif")
  ## gg1A <- animate(gg1,renderer=ffmpeg_renderer())
}
```

## random peak thoughts

From @joschabach "deadly delusion" post on Medium.

Suppose 

- the US limit is 170K critically ill patients
- 6% of cases are critically ill
- US has 3e8 people
- so we need to stay below 1.7e5*/0.06/3e8 ~ 1% of the population ill

```{r}
uniroot(function(x) peak_I(x)-0.01,interval=c(1,2))
```

- peak = (R0-1)*0.15
- peak ICU: 3e8*0.06*peak
- 10% reduction in R0 = reduction of 0.2*0.15*3e7*0.06
